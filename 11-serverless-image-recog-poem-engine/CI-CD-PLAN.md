# Project 11 CI/CD Deployment Plan

## ğŸ“‹ Project Analysis

**Project Type:** Serverless Image Recognition + AI Poetry Generation
**Key Components:**
- 2 Lambda functions (upload handler + image processor)
- S3 bucket with event triggers
- AWS Rekognition for image analysis
- AWS Bedrock for AI poem generation
- Terraform infrastructure

## ğŸ—ï¸ Architecture Flow

```
User Upload â†’ S3 Bucket â†’ Lambda Trigger â†’ Rekognition â†’ Bedrock â†’ Poem Storage
     â†‘                                                                    â†“
Upload Lambda (Presigned URLs) â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â† Cleanup
```

## ğŸš€ CI/CD Pipeline Jobs

### Job 1: `package-lambda-functions`
**Purpose:** Package both Lambda functions into zip files
**Dependencies:** None
**Actions:**
- Checkout code
- Setup Python 3.12
- Package upload Lambda â†’ `lambda_upload.zip`
- Package image_recog Lambda â†’ `lambda_image_recog.zip`
- Upload both as artifacts

### Job 2: `deploy-infrastructure`
**Purpose:** Deploy all AWS resources using Terraform
**Dependencies:** `package-lambda-functions`
**Actions:**
- Checkout code
- Download Lambda artifacts
- Setup Terraform
- Deploy infrastructure (S3, IAM, Lambda functions, triggers)
- Capture outputs (bucket name, function URLs)

### Job 3: `destroy-infrastructure` (conditional)
**Purpose:** Clean up all resources when action = 'destroy'
**Dependencies:** None (runs independently)
**Actions:**
- Checkout code
- Setup Terraform
- Destroy all resources

## ğŸ”§ Key Differences from Project 10

| Aspect | Project 10 | Project 11 |
|--------|------------|------------|
| **Containers** | Docker + ECR | None |
| **Lambda Functions** | 1 function | 2 functions |
| **AI Services** | None | Rekognition + Bedrock |
| **Triggers** | Manual/Kinesis | S3 Events |
| **Complexity** | High (multi-stage) | Medium (simpler) |

## ğŸ“¦ Terraform Resources

**Infrastructure includes:**
- S3 bucket with lifecycle rules
- 2 Lambda functions with different roles
- IAM policies for Rekognition + Bedrock
- S3 event notifications
- Lambda function URLs with CORS
- CloudWatch logging

## ğŸ¯ Deployment Variables

**Required Terraform Variables:**
- `project_name` - Unique identifier
- `aws_region` - Deployment region
- `bedrock_model_id` - AI model selection

**Environment Variables for Lambda:**
- `BUCKET_NAME` - S3 bucket name
- `BEDROCK_MODEL_ID` - AI model ID
- `LOG_LEVEL` - Logging level

## âš¡ Simplified Pipeline (vs Project 10)

**Project 10 Flow:**
```
ECR Setup â†’ Docker Build â†’ Lambda Package â†’ Infrastructure â†’ Cleanup
```

**Project 11 Flow:**
```
Lambda Package â†’ Infrastructure Deploy/Destroy
```

## ğŸ” Testing Strategy

**Manual Testing Commands:**
```bash
# Test upload function
curl -X POST https://lambda-url/upload -d '{"fileName":"test.jpg"}'

# Test image processing (upload image to S3)
aws s3 cp test-image.jpg s3://bucket-name/uploads/

# Check generated poems
aws s3 ls s3://bucket-name/poems/
```

## ğŸ“ GitHub Actions Structure

```yaml
name: Project 11 - Serverless Image Recognition Poem Engine

on:
  workflow_dispatch:
    inputs:
      action: [deploy, destroy]

env:
  AWS_REGION: "ap-south-1"
  PROJECT_NAME: "11-serverless-image-recog-poem"

jobs:
  package-lambda-functions:    # Package both Lambda functions
  deploy-infrastructure:       # Deploy via Terraform
  destroy-infrastructure:      # Cleanup (conditional)
```

## ğŸ¯ Success Criteria

**Deployment Success:**
- âœ… Both Lambda functions deployed
- âœ… S3 bucket created with proper triggers
- âœ… Upload function returns presigned URLs
- âœ… Image processing generates poems
- âœ… Automatic cleanup works

**Testing Success:**
- âœ… Upload image â†’ poem generated
- âœ… Multiple images processed
- âœ… Error handling works
- âœ… Cleanup removes uploaded images

This is a much simpler pipeline than Project 10 - no Docker, no ECR, just Lambda packaging and Terraform deployment!